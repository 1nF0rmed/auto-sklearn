{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Obtain run information\n\nThe following example shows how to obtain information from a finished\nAuto-sklearn run. In particular, it shows:\n* how to query which models were evaluated by Auto-sklearn\n* how to query the models in the final ensemble\n* how to get general statistics on the what Auto-sklearn evaluated\n\nAuto-sklearn is a wrapper on top of\nthe sklearn models. This example illustrates how to interact\nwith the sklearn components directly, in this case a PCA preprocessor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sklearn.datasets\nimport sklearn.metrics\n\nimport autosklearn.classification\n\n\nif __name__ == \"__main__\":\n    ############################################################################\n    # Data Loading\n    # ============\n\n    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    X_train, X_test, y_train, y_test = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=1)\n\n    ############################################################################\n    # Build and fit the classifier\n    # ============================\n\n    automl = autosklearn.classification.AutoSklearnClassifier(\n        time_left_for_this_task=30,\n        per_run_time_limit=10,\n        disable_evaluator_output=False,\n        # To simplify querying the models in the final ensemble, we\n        # restrict auto-sklearn to use only pca as a preprocessor\n        include_preprocessors=['pca'],\n    )\n    automl.fit(X_train, y_train, dataset_name='breast_cancer')\n\n    ############################################################################\n    # Predict using the model\n    # =======================\n\n    predictions = automl.predict(X_test)\n    print(\"Accuracy score:{}\".format(\n        sklearn.metrics.accuracy_score(y_test, predictions))\n    )\n\n\n    ############################################################################\n    # Report the models found by Auto-Sklearn\n    # =======================================\n    #\n    # Auto-sklearn uses\n    # `Ensemble Selection <https://www.cs.cornell.edu/~alexn/papers/shotgun.icml04.revised.rev2.pdf>`_\n    # to construct ensembles in a post-hoc fashion. The ensemble is a linear\n    # weighting of all models constructed during the hyperparameter optimization.\n    # This prints the final ensemble. It is a list of tuples, each tuple being\n    # the model weight in the ensemble and the model itself.\n\n    print(automl.show_models())\n\n    ###########################################################################\n    # Report statistics about the search\n    # ==================================\n    #\n    # Print statistics about the auto-sklearn run such as number of\n    # iterations, number of models failed with a time out etc.\n    print(automl.sprint_statistics())\n\n    ############################################################################\n    # Detailed statistics about the search - part 1\n    # =============================================\n    #\n    # Auto-sklearn also keeps detailed statistics of the hyperparameter\n    # optimization procedurce, which are stored in a so-called\n    # `run history <https://automl.github.io/SMAC3/master/apidoc/smac.\n    # runhistory.runhistory.html#smac.runhistory# .runhistory.RunHistory>`_.\n\n    print(automl.automl_.runhistory_)\n\n    ############################################################################\n    # Runs are stored inside an ``OrderedDict`` called ``data``:\n\n    print(len(automl.automl_.runhistory_.data))\n\n    ############################################################################\n    # Let's iterative over all entries\n\n    for run_key in automl.automl_.runhistory_.data:\n        print('#########')\n        print(run_key)\n        print(automl.automl_.runhistory_.data[run_key])\n\n    ############################################################################\n    # and have a detailed look at one entry:\n\n    run_key = list(automl.automl_.runhistory_.data.keys())[0]\n    run_value = automl.automl_.runhistory_.data[run_key]\n\n    ############################################################################\n    # The ``run_key`` contains all information describing a run:\n\n    print(\"Configuration ID:\", run_key.config_id)\n    print(\"Instance:\", run_key.instance_id)\n    print(\"Seed:\", run_key.seed)\n    print(\"Budget:\", run_key.budget)\n\n    ############################################################################\n    # and the configuration can be looked up in the run history as well:\n\n    print(automl.automl_.runhistory_.ids_config[run_key.config_id])\n\n    ############################################################################\n    # The only other important entry is the budget in case you are using\n    # auto-sklearn with\n    # `successive halving <../60_search/example_successive_halving.html>`_.\n    # The remaining parts of the key can be ignored for auto-sklearn and are\n    # only there because the underlying optimizer, SMAC, can handle more general\n    # problems, too.\n\n    ############################################################################\n    # The ``run_value`` contains all output from running the configuration:\n\n    print(\"Cost:\", run_value.cost)\n    print(\"Time:\", run_value.time)\n    print(\"Status:\", run_value.status)\n    print(\"Additional information:\", run_value.additional_info)\n    print(\"Start time:\", run_value.starttime)\n    print(\"End time\", run_value.endtime)\n\n    ############################################################################\n    # Cost is basically the same as a loss. In case the metric to optimize for\n    # should be maximized, it is internally transformed into a minimization\n    # metric. Additionally, the status type gives information on whether the run\n    # was successful, while the additional information's most interesting entry\n    # is the internal training loss. Furthermore, there is detailed information\n    # on the runtime available.\n\n    ############################################################################\n    # As an example, let's find the best configuration evaluated. As\n    # Auto-sklearn solves a minimization problem internally, we need to look\n    # for the entry with the lowest loss:\n\n    losses_and_configurations = [\n        (run_value.cost, run_key.config_id)\n        for run_key, run_value in automl.automl_.runhistory_.data.items()\n    ]\n    losses_and_configurations.sort()\n    print(\"Lowest loss:\", losses_and_configurations[0][0])\n    print(\n        \"Best configuration:\",\n        automl.automl_.runhistory_.ids_config[losses_and_configurations[0][1]]\n    )\n\n    ############################################################################\n    # Detailed statistics about the search - part 2\n    # =============================================\n    #\n    # To maintain compatibility with scikit-learn, Auto-sklearn gives the\n    # same data as\n    # `cv_results_ <https://scikit-learn.org/stable/modules/generated/sklearn.\n    # model_selection.GridSearchCV.html>`_.\n\n    print(automl.cv_results_)\n\n    ############################################################################\n    # Inspect the components of the best model\n    # ========================================\n    #\n    # Iterate over the components of the model and print\n    # The explained variance ratio per stage\n    for i, (weight, pipeline) in enumerate(automl.get_models_with_weights()):\n        for stage_name, component in pipeline.named_steps.items():\n            if 'preprocessor' in stage_name:\n                print(\n                    \"The {}th pipeline has a explained variance of {}\".format(\n                        i,\n                        # The component is an instance of AutoSklearnChoice.\n                        # Access the sklearn object via the choice attribute\n                        # We want the explained variance attributed of\n                        # each principal component\n                        component.choice.preprocessor.explained_variance_ratio_\n                    )\n                )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}