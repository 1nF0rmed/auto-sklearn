
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/40_advanced/example_metrics.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_40_advanced_example_metrics.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_40_advanced_example_metrics.py:


=======
Metrics
=======

*Auto-sklearn* supports various built-in metrics, which can be found in the
:ref:`metrics section in the API <api:Built-in Metrics>`. However, it is also
possible to define your own metric and use it to fit and evaluate your model.
The following examples show how to use built-in and self-defined metrics for a
classification problem.

.. GENERATED FROM PYTHON SOURCE LINES 13-24

.. code-block:: default


    import numpy as np

    import sklearn.model_selection
    import sklearn.datasets
    import sklearn.metrics

    import autosklearn.classification
    import autosklearn.metrics









.. GENERATED FROM PYTHON SOURCE LINES 25-30

Data Loading
============
The custom metrics must be in a separate module to be usable together with
Auto-sklearn. We also print the content of the module below with
``inspect`` to keep the example self-contained.

.. GENERATED FROM PYTHON SOURCE LINES 30-183

.. code-block:: default


    import custom_metrics


    if __name__ == "__main__":
        import inspect

        print(inspect.getsource(custom_metrics))

        ############################################################################
        # Data Loading
        # ============

        X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)
        X_train, X_test, y_train, y_test = \
            sklearn.model_selection.train_test_split(X, y, random_state=1)

        ############################################################################
        # Print a list of available metrics
        # =================================

        print("Available CLASSIFICATION metrics autosklearn.metrics.*:")
        print("\t*" + "\n\t*".join(autosklearn.metrics.CLASSIFICATION_METRICS))

        print("Available REGRESSION autosklearn.metrics.*:")
        print("\t*" + "\n\t*".join(autosklearn.metrics.REGRESSION_METRICS))

        ############################################################################
        # First example: Use predefined accuracy metric
        # =============================================

        print("#"*80)
        print("Use predefined accuracy metric")
        cls = autosklearn.classification.AutoSklearnClassifier(
            time_left_for_this_task=60,
            per_run_time_limit=30,
            seed=1,
            metric=autosklearn.metrics.accuracy,
        )
        cls.fit(X_train, y_train)

        predictions = cls.predict(X_test)
        print("Accuracy score {:g} using {:s}".
              format(sklearn.metrics.accuracy_score(y_test, predictions),
                     cls.automl_._metric.name))

        ############################################################################
        # Second example: Use own accuracy metric
        # =======================================

        print("#"*80)
        print("Use self defined accuracy metric")
        accuracy_scorer = autosklearn.metrics.make_scorer(
            name="accu",
            score_func=custom_metrics.accuracy,
            optimum=1,
            greater_is_better=True,
            needs_proba=False,
            needs_threshold=False,
        )
        cls = autosklearn.classification.AutoSklearnClassifier(
            time_left_for_this_task=60,
            per_run_time_limit=30,
            seed=1,
            metric=accuracy_scorer,
        )
        cls.fit(X_train, y_train)

        predictions = cls.predict(X_test)
        print("Accuracy score {:g} using {:s}".
              format(sklearn.metrics.accuracy_score(y_test, predictions),
                     cls.automl_._metric.name))

        print("#"*80)
        print("Use self defined error metric")
        error_rate = autosklearn.metrics.make_scorer(
            name='error',
            score_func=custom_metrics.error,
            optimum=0,
            greater_is_better=False,
            needs_proba=False,
            needs_threshold=False
        )
        cls = autosklearn.classification.AutoSklearnClassifier(
            time_left_for_this_task=60,
            per_run_time_limit=30,
            seed=1,
            metric=error_rate,
        )
        cls.fit(X_train, y_train)

        cls.predictions = cls.predict(X_test)
        print("Error rate {:g} using {:s}".
              format(error_rate(y_test, predictions),
                     cls.automl_._metric.name))

        ############################################################################
        # Third example: Use own accuracy metric with additional argument
        # ===============================================================

        print("#"*80)
        print("Use self defined accuracy with additional argument")
        accuracy_scorer = autosklearn.metrics.make_scorer(
            name="accu_add",
            score_func=custom_metrics.accuracy_wk,
            optimum=1,
            greater_is_better=True,
            needs_proba=False,
            needs_threshold=False,
            dummy=None,
        )
        cls = autosklearn.classification.AutoSklearnClassifier(
            time_left_for_this_task=60,
            per_run_time_limit=30,
            seed=1,
            metric=accuracy_scorer
        )
        cls.fit(X_train, y_train)

        predictions = cls.predict(X_test)
        print(
            "Accuracy score {:g} using {:s}".format(
                sklearn.metrics.accuracy_score(y_test, predictions),
                cls.automl_._metric.name
            )
        )

        print("#"*80)
        print("Use self defined error with additional argument")
        error_rate = autosklearn.metrics.make_scorer(
            name="error_add",
            score_func=custom_metrics.error_wk,
            optimum=0,
            greater_is_better=True,
            needs_proba=False,
            needs_threshold=False,
            dummy=None,
        )
        cls = autosklearn.classification.AutoSklearnClassifier(
            time_left_for_this_task=60,
            per_run_time_limit=30,
            seed=1,
            metric=error_rate,
        )
        cls.fit(X_train, y_train)

        predictions = cls.predict(X_test)
        print(
            "Error rate {:g} using {:s}".format(
                error_rate(y_test, predictions),
                cls.automl_._metric.name
            )
        )




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    """Custom metrics to be used by example_metrics.py

    They reside in a different file so they can be used by Auto-sklearn."""

    import numpy as np


    ############################################################################
    # Custom metrics definition
    # =========================

    def accuracy(solution, prediction):
        # custom function defining accuracy
        return np.mean(solution == prediction)


    def error(solution, prediction):
        # custom function defining error
        return np.mean(solution != prediction)


    def accuracy_wk(solution, prediction, dummy):
        # custom function defining accuracy and accepting an additional argument
        assert dummy is None
        return np.mean(solution == prediction)


    def error_wk(solution, prediction, dummy):
        # custom function defining error and accepting an additional argument
        assert dummy is None
        return np.mean(solution != prediction)

    Available CLASSIFICATION metrics autosklearn.metrics.*:
            *accuracy
            *balanced_accuracy
            *roc_auc
            *average_precision
            *log_loss
            *precision
            *precision_macro
            *precision_micro
            *precision_samples
            *precision_weighted
            *recall
            *recall_macro
            *recall_micro
            *recall_samples
            *recall_weighted
            *f1
            *f1_macro
            *f1_micro
            *f1_samples
            *f1_weighted
    Available REGRESSION autosklearn.metrics.*:
            *mean_absolute_error
            *mean_squared_error
            *root_mean_squared_error
            *mean_squared_log_error
            *median_absolute_error
            *r2
    ################################################################################
    Use predefined accuracy metric
    Accuracy score 0.944056 using accuracy
    ################################################################################
    Use self defined accuracy metric
    Accuracy score 0.958042 using accu
    ################################################################################
    Use self defined error metric
    Error rate -0.041958 using error
    ################################################################################
    Use self defined accuracy with additional argument
    Accuracy score 0.958042 using accu_add
    ################################################################################
    Use self defined error with additional argument
    [WARNING] [2021-01-05 09:45:26,823:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:45:30,557:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:45:34,463:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:45:36,571:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:45:39,426:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:45:40,860:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:45:44,025:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:45:48,811:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:45:50,209:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:45:55,301:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:46:00,737:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:46:06,715:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    [WARNING] [2021-01-05 09:46:13,061:smac.runhistory.runhistory2epm.RunHistory2EPM4LogCost] Got cost of smaller/equal to 0. Replace by 0.000010 since we use log cost.
    Error rate 0.615385 using error_add





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 5 minutes  4.858 seconds)


.. _sphx_glr_download_examples_40_advanced_example_metrics.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: example_metrics.py <example_metrics.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: example_metrics.ipynb <example_metrics.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
