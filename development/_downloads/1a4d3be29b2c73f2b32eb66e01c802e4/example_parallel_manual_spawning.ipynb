{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Parallel Usage with manual process spawning\n\n*Auto-sklearn* uses\n`dask.distributed <https://distributed.dask.org/en/latest/index.html`>_\nfor parallel optimization.\n\nThis example shows how to spawn workers for *Auto-sklearn* manually.\nUse this example as a starting point to parallelize *Auto-sklearn*\nacross multiple machines. To run *Auto-sklearn* in parallel\non a single machine check out the example\n`Parallel Usage on a single machine <example_parallel_n_jobs.html>`_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\nimport multiprocessing\nimport subprocess\nimport time\n\nimport dask\nimport dask.distributed\nimport sklearn.datasets\nimport sklearn.metrics\n\nfrom autosklearn.classification import AutoSklearnClassifier\nfrom autosklearn.constants import MULTICLASS_CLASSIFICATION\n\ntmp_folder = '/tmp/autosklearn_parallel_2_example_tmp'\noutput_folder = '/tmp/autosklearn_parallel_2_example_out'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dask configuration\n\nAuto-sklearn uses threads in Dask to launch memory constrained jobs.\nThis number of threads can be provided directly via the n_jobs argument\nwhen creating the AutoSklearnClassifier. Additionally, the user can provide\na dask_client argument which can have processes=True.\nWhen using processes to True, we need to specify the below setting\nto allow internally generated processes.\nOptionally, you can choose to provide a dask client with processes=False\nand remove the following line.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dask.config.set({'distributed.worker.daemon': False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start worker - Python\n\nThis function demonstrates how to start a dask worker from python. This\nis a bit cumbersome and should ideally be done from the command line.\nWe do it here for illustrational purpose, butalso start one worker from\nthe command line below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check the dask docs at\n# https://docs.dask.org/en/latest/setup/python-advanced.html for further\n# information.\n\n\ndef start_python_worker(scheduler_address):\n    dask.config.set({'distributed.worker.daemon': False})\n\n    async def do_work():\n        async with dask.distributed.Nanny(\n                scheduler_ip=scheduler_address,\n                nthreads=1,\n                lifetime=35,  # automatically shut down the worker so this loop ends\n        ) as worker:\n            await worker.finished()\n\n    asyncio.get_event_loop().run_until_complete(do_work())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start worker - CLI\n\nIt is also possible to start dask workers from the command line (in fact,\none can also start a dask scheduler from the command line), see the\n`dask cli docs <https://docs.dask.org/en/latest/setup/cli.html>`_ for\nfurther information.\nPlease not, that DASK_DISTRIBUTED__WORKER__DAEMON=False is required in this\ncase as dask-worker creates a new process. That is, it is equivalent to the\nsetting described above with dask.distributed.Client with processes=True\n\nAgain, we need to make sure that we do not start the workers in a daemon\nmode.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def start_cli_worker(scheduler_address):\n    call_string = (\n        \"DASK_DISTRIBUTED__WORKER__DAEMON=False \"\n        \"dask-worker %s --nthreads 1 --lifetime 35\"\n    ) % scheduler_address\n    proc = subprocess.run(call_string, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE, shell=True, check=True)\n    while proc.returncode is None:\n        time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Auto-sklearn\n\nWe are now ready to start *auto-sklearn.\n\nTo use auto-sklearn in parallel we must guard the code with\n``if __name__ == '__main__'``. We then start a dask cluster as a context,\nwhich means that it is automatically stopped one all computation is done.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    X_train, X_test, y_train, y_test = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=1)\n\n    # Create a dask compute cluster and a client manually - the former can\n    # be done via command line, too.\n    with dask.distributed.LocalCluster(\n        n_workers=0, processes=True, threads_per_worker=1,\n    ) as cluster, dask.distributed.Client(address=cluster.scheduler_address) as client:\n\n        # now we start the two workers, one from within Python, the other\n        # via the command line.\n        process_python_worker = multiprocessing.Process(\n            target=start_python_worker,\n            args=(cluster.scheduler_address,),\n        )\n        process_python_worker.start()\n        process_cli_worker = multiprocessing.Process(\n            target=start_cli_worker,\n            args=(cluster.scheduler_address,),\n        )\n        process_cli_worker.start()\n\n        # Wait a second for workers to become available\n        time.sleep(1)\n\n        automl = AutoSklearnClassifier(\n            time_left_for_this_task=30,\n            per_run_time_limit=10,\n            memory_limit=1024,\n            tmp_folder=tmp_folder,\n            output_folder=output_folder,\n            seed=777,\n            # n_jobs is ignored internally as we pass a dask client.\n            n_jobs=1,\n            # Pass a dask client which connects to the previously constructed cluster.\n            dask_client=client,\n        )\n        automl.fit(X_train, y_train)\n\n        automl.fit_ensemble(\n            y_train,\n            task=MULTICLASS_CLASSIFICATION,\n            dataset_name='digits',\n            ensemble_size=20,\n            ensemble_nbest=50,\n        )\n\n        predictions = automl.predict(X_test)\n        print(automl.sprint_statistics())\n        print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, predictions))\n\n        # Wait until all workers are closed\n        process_python_worker.join()\n        process_cli_worker.join()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}