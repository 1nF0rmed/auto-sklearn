{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Test and Train data with Pandas\n\n*auto-sklearn* can automatically encode categorical columns using a label/ordinal encoder.\nThis example highlights how to properly set the dtype in a DataFrame for this to happen,\nand showcase how to input also testing data to autosklearn.\nThe X_train/y_train arguments to the fit function will be used to fit the scikit-learn model,\nwhereas the X_test/y_test will be used to evaluate how good this scikit-learn model generalizes\nto unseen data (i.e. data not in X_train/y_train). Using test data is a good mechanism to measure\nif the trained model suffers from overfit, and more details can be found on `evaluating estimator\nperformance <https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation>`_.\nThis example further highlights through a plot, the best individual models found by *auto-sklearn*\nthrough time (under single_best_optimization_score/single_best_test_score's legend).\nIt also shows the training and test performance of the ensemble build using the best\nperforming models (under ensemble_optimization_score and ensemble_test_score respectively).\n\nThere is also support to manually indicate the feature types (whether a column is categorical\nor numerical) via the argument feat_types from fit(). This is important when working with\nlist or numpy arrays as there is no per-column dtype (further details in the example\n`Continuous and categorical data <example_feature_types.html>`_).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\n\nfrom smac.tae import StatusType\n\nimport autosklearn.classification\n\n\ndef get_runhistory_models_performance(automl):\n    metric = cls.automl_._metric\n    data = automl.automl_.runhistory_.data\n    performance_list = []\n    for run_key, run_value in data.items():\n        if run_value.status != StatusType.SUCCESS:\n            # Ignore crashed runs\n            continue\n        # Alternatively, it is possible to also obtain the start time with ``run_value.starttime``\n        endtime = pd.Timestamp(time.strftime('%Y-%m-%d %H:%M:%S',\n                                             time.localtime(run_value.endtime)))\n        val_score = metric._optimum - (metric._sign * run_value.cost)\n        test_score = metric._optimum - (metric._sign * run_value.additional_info['test_loss'])\n        train_score = metric._optimum - (metric._sign * run_value.additional_info['train_loss'])\n        performance_list.append({\n            'Timestamp': endtime,\n            'single_best_optimization_score': val_score,\n            'single_best_test_score': test_score,\n            'single_best_train_score': train_score,\n        })\n    return pd.DataFrame(performance_list)\n\n\nif __name__ == \"__main__\":\n    ############################################################################\n    # Data Loading\n    # ============\n\n    # Using Australian dataset https://www.openml.org/d/40981.\n    # This example will use the command fetch_openml, which will\n    # download a properly formatted dataframe if you use as_frame=True.\n    # For demonstration purposes, we will download a numpy array using\n    # as_frame=False, and manually creating the pandas DataFrame\n    X, y = sklearn.datasets.fetch_openml(data_id=40981, return_X_y=True, as_frame=False)\n\n    # bool and category will be automatically encoded.\n    # Targets for classification are also automatically encoded\n    # If using fetch_openml, data is already properly encoded, below\n    # is an example for user reference\n    X = pd.DataFrame(\n        data=X,\n        columns=['A' + str(i) for i in range(1, 15)]\n    )\n    desired_boolean_columns = ['A1']\n    desired_categorical_columns = ['A4', 'A5', 'A6', 'A8', 'A9', 'A11', 'A12']\n    desired_numerical_columns = ['A2', 'A3', 'A7', 'A10', 'A13', 'A14']\n    for column in X.columns:\n        if column in desired_boolean_columns:\n            X[column] = X[column].astype('bool')\n        elif column in desired_categorical_columns:\n            X[column] = X[column].astype('category')\n        else:\n            X[column] = pd.to_numeric(X[column])\n\n    y = pd.DataFrame(y, dtype='category')\n\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n        X, y, test_size=0.5, random_state=3\n    )\n    print(X.dtypes)\n\n    ############################################################################\n    # Build and fit a classifier\n    # ==========================\n\n    cls = autosklearn.classification.AutoSklearnClassifier(\n        time_left_for_this_task=120,\n        per_run_time_limit=30,\n    )\n    cls.fit(X_train, y_train, X_test, y_test)\n\n    ###########################################################################\n    # Get the Score of the final ensemble\n    # ===================================\n\n    predictions = cls.predict(X_test)\n    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, predictions))\n\n    ############################################################################\n    # Plot the ensemble performance\n    # ===================================\n\n    ensemble_performance_frame = pd.DataFrame(cls.automl_.ensemble_performance_history)\n    best_values = pd.Series({'ensemble_optimization_score': -np.inf,\n                             'ensemble_test_score': -np.inf})\n    for idx in ensemble_performance_frame.index:\n        if (\n            ensemble_performance_frame.loc[idx, 'ensemble_optimization_score']\n            > best_values['ensemble_optimization_score']\n        ):\n            best_values = ensemble_performance_frame.loc[idx]\n        ensemble_performance_frame.loc[idx] = best_values\n\n    individual_performance_frame = get_runhistory_models_performance(cls)\n    best_values = pd.Series({'single_best_optimization_score': -np.inf,\n                             'single_best_test_score': -np.inf,\n                             'single_best_train_score': -np.inf})\n    for idx in individual_performance_frame.index:\n        if (\n            individual_performance_frame.loc[idx, 'single_best_optimization_score']\n            > best_values['single_best_optimization_score']\n        ):\n            best_values = individual_performance_frame.loc[idx]\n        individual_performance_frame.loc[idx] = best_values\n\n    pd.merge(\n        ensemble_performance_frame,\n        individual_performance_frame,\n        on=\"Timestamp\", how='outer'\n    ).sort_values('Timestamp').fillna(method='ffill').plot(\n        x='Timestamp',\n        kind='line',\n        legend=True,\n        title='Auto-sklearn accuracy over time',\n        grid=True,\n    )\n    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}